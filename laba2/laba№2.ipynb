{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install msgpack\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Вариант №36***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание №1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нормализованная матрица сохранена в ./data/first_task_normalized.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_npy(file_path, output_json_file, normalized_file):\n",
    "    matrix = np.load(file_path)\n",
    "    total_sum = np.sum(matrix)\n",
    "    total_avg = np.mean(matrix)\n",
    "    main_diagonal = np.diagonal(matrix)\n",
    "    sum_main_diag = np.sum(main_diagonal)\n",
    "    avg_main_diag = np.mean(main_diagonal)\n",
    "    secondary_diagonal = np.fliplr(matrix).diagonal()\n",
    "    sum_secondary_diag = np.sum(secondary_diagonal)\n",
    "    avg_secondary_diag = np.mean(secondary_diagonal)\n",
    "    max_val = np.max(matrix)\n",
    "    min_val = np.min(matrix)\n",
    "\n",
    "    open(output_json_file, \"w\").write(\n",
    "        f\"\"\"\n",
    "    {{\n",
    "        \"sum\": {total_sum},\n",
    "        \"avr\": {total_avg},\n",
    "        \"sumMD\": {sum_main_diag},\n",
    "        \"avrMD\": {avg_main_diag},\n",
    "        \"sumSD\": {sum_secondary_diag},\n",
    "        \"avrSD\": {avg_secondary_diag},\n",
    "        \"max\": {max_val},\n",
    "        \"min\": {min_val}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    min_matrix = matrix.min()\n",
    "    max_matrix = matrix.max()\n",
    "    normalized_matrix = (matrix - min_matrix) / (max_matrix - min_matrix)\n",
    "\n",
    "    np.save(normalized_file, normalized_matrix)\n",
    "    print(f\"Нормализованная матрица сохранена в {normalized_file}\")\n",
    "\n",
    "\n",
    "normalized_file = \"first_task_normalized.npy\"\n",
    "file_path = \"first_task.npy\"\n",
    "update_json_file = \"first_task_results.json\"\n",
    "folder = \"./data/\"\n",
    "process_npy(folder + file_path, folder + update_json_file, folder + normalized_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание №2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of uncompressed npz file: 306226 bytes\n",
      "Size of compressed npz file: 37760 bytes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "folder = \"./data/\"\n",
    "\n",
    "matrix = np.load(folder + file_path)\n",
    "indices_i = []\n",
    "indices_j = []\n",
    "values = []\n",
    "\n",
    "for i in range(matrix.shape[0]):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        if matrix[i, j] > matrix[i, (j + 1) % matrix.shape[1]]:\n",
    "            indices_i.append(i)\n",
    "            indices_j.append(j)\n",
    "            values.append(matrix[i, j])\n",
    "\n",
    "indices_i = np.array(indices_i)\n",
    "indices_j = np.array(indices_j)\n",
    "values = np.array(values)\n",
    "\n",
    "np.savez(folder + 'second_arrays.npz', indices_i=indices_i, indices_j=indices_j, values=values)\n",
    "np.savez_compressed(folder + 'second_arrays_compressed.npz', indices_i=indices_i, indices_j=indices_j, values=values)\n",
    "size_uncompressed = os.path.getsize(folder + \"second_arrays.npz\")\n",
    "size_compressed = os.path.getsize(folder + 'second_arrays_compressed.npz')\n",
    "\n",
    "print(f\"Size of uncompressed npz file: {size_uncompressed} bytes\")\n",
    "print(f\"Size of compressed npz file: {size_compressed} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание №3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of JSON file: 7379 bytes\n",
      "Size of Msgpack file: 5891 bytes\n"
     ]
    }
   ],
   "source": [
    "import msgpack\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def process_npy(folder, file_json_path, output_json_file, output_msgpack_file):\n",
    "    data = pd.read_json(folder + file_json_path)\n",
    "    aggregated_data = (\n",
    "        data.groupby(\"name\")\n",
    "        .agg(\n",
    "            avg_price=(\"price\", \"mean\"),\n",
    "            max_price=(\"price\", \"max\"),\n",
    "            min_price=(\"price\", \"min\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    aggregated_data.to_json(folder + output_json_file, force_ascii=False, orient='records')\n",
    "\n",
    "    with open(folder + output_msgpack_file, 'wb') as f:\n",
    "        packed = msgpack.packb(\n",
    "            aggregated_data.to_dict(orient=\"records\"), use_bin_type=True\n",
    "        )\n",
    "        f.write(packed)\n",
    "\n",
    "    size_json = os.path.getsize(folder + output_json_file)\n",
    "    size_msgpack = os.path.getsize(folder + output_msgpack_file)\n",
    "\n",
    "    print(f\"Size of JSON file: {size_json} bytes\")\n",
    "    print(f\"Size of Msgpack file: {size_msgpack} bytes\")\n",
    "\n",
    "folder = \"./data/\"\n",
    "file_json_path = \"third_task.json\"\n",
    "update_json_file = \"third_task_results.json\"\n",
    "output_msgpack_file = \"third_task_results.msgpack\"\n",
    "process_npy(folder, file_json_path, update_json_file, output_msgpack_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание №4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "method = {\n",
    "        \"add\": lambda x, y: x + y,\n",
    "        \"sub\": lambda x, y: x - y,\n",
    "        \"percent+\": lambda x, y: x * (1 + (float(y) / 100)),\n",
    "        \"percent-\": lambda x, y: x * (1 - (float(y) / 100)),\n",
    "}\n",
    "\n",
    "\n",
    "def process_npy(folder, file_pkl_path, update_json_file, output_pkl_file):\n",
    "    products = pickle.load(open(folder + file_pkl_path, \"rb\"))\n",
    "    updates = pd.read_json(folder + update_json_file)\n",
    "    for _, row in updates.iterrows():\n",
    "        product_name = row['name']\n",
    "        if product_name in products:\n",
    "            products[product_name]['price'] = method[row['method']](products[product_name]['price'], row['param'])\n",
    "    with open(folder + output_pkl_file, 'wb') as f:\n",
    "        pickle.dump(products, f)\n",
    "\n",
    "folder = \"./data/\"\n",
    "file_pkl_path = \"fourth_task_products.pkl\"\n",
    "update_json_file = \"fourth_task_updates.json\"\n",
    "output_pkl_file = \"fourth_task_products_results.pkl\"\n",
    "process_npy(folder, file_pkl_path, update_json_file, output_pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание №5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of CSV file: 4594853 bytes\n",
      "Size of JSON file: 26204301 bytes\n",
      "Size of Msgpack file: 22696794 bytes\n",
      "Size of PKL file: 8836234 bytes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import msgpack\n",
    "import pandas as pd\n",
    "from numpy import int64, float64\n",
    "\n",
    "def process_npy(folder, file_csv_path, output_csv_file, output_json_file, output_msgpack_file, output_pkl_file):\n",
    "    df = pd.read_csv(folder + file_csv_path)\n",
    "    selected_fields = ['hotel', 'is_canceled', 'lead_time', 'arrival_date_year', 'arrival_date_week_number', 'arrival_date_day_of_month', 'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children']\n",
    "    df_selected = df[selected_fields]\n",
    "    numerical_fields = df_selected.select_dtypes(include=[int64, float64]).columns\n",
    "    stats = {}\n",
    "    for field in numerical_fields:\n",
    "        stats[field] = {\n",
    "            'max': df_selected[field].max(),\n",
    "            'min': df_selected[field].min(),\n",
    "            'mean': df_selected[field].mean(),\n",
    "            'sum': df_selected[field].sum(),\n",
    "            'std': df_selected[field].std()\n",
    "        }\n",
    "    categorical_fields = df_selected.select_dtypes(include=[object]).columns\n",
    "    for field in categorical_fields:\n",
    "        stats[field] = df_selected[field].value_counts().to_dict()\n",
    "\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, int64):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, float64):\n",
    "            return float(obj)\n",
    "        return obj\n",
    "\n",
    "    with open(folder + output_json_file, 'w') as f:\n",
    "        json.dump(stats, f, ensure_ascii=False, indent=4, default=convert_to_serializable)\n",
    "\n",
    "    # Save the selected DataFrame to different formats\n",
    "    df_selected.to_csv(folder + output_csv_file, index=False)\n",
    "    df_selected.to_json(folder + output_json_file, orient='records', force_ascii=False)\n",
    "    with open(folder + output_msgpack_file, 'wb') as f:\n",
    "        packed = msgpack.packb(df_selected.to_dict(orient='records'), use_bin_type=True)\n",
    "        f.write(packed)\n",
    "    with open(folder + output_pkl_file, 'wb') as f:\n",
    "        pickle.dump(df_selected, f)\n",
    "\n",
    "    size_csv = os.path.getsize(folder + output_csv_file)\n",
    "    size_json = os.path.getsize(folder + output_json_file)\n",
    "    size_msgpack = os.path.getsize(folder + output_msgpack_file)\n",
    "    size_pkl = os.path.getsize(folder + output_pkl_file)\n",
    "\n",
    "    print(f\"Size of CSV file: {size_csv} bytes\")\n",
    "    print(f\"Size of JSON file: {size_json} bytes\")\n",
    "    print(f\"Size of Msgpack file: {size_msgpack} bytes\")\n",
    "    print(f\"Size of PKL file: {size_pkl} bytes\")\n",
    "\n",
    "\n",
    "\n",
    "folder = \"./data/\"\n",
    "file_csv_path = \"hotel_booking.csv\"\n",
    "output_csv_file = \"five_results.csv\"\n",
    "output_json_file = \"five_results.json\"\n",
    "output_msgpack_file = \"five_results.msgpack\"\n",
    "output_pkl_file = \"five_results.pkl\"\n",
    "process_npy(folder, file_csv_path, output_csv_file, output_json_file, output_msgpack_file, output_pkl_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
